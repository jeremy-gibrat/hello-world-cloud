# Hello World - Kubernetes avec Helm

Application complÃ¨te dÃ©ployÃ©e sur Minikube ou Azure AKS avec backend Java Spring Boot, frontend Angular, RabbitMQ et stack ELK (Elasticsearch, Logstash, Kibana).

## ğŸ“‹ PrÃ©requis

- Docker avec buildx (multi-platform)
- Minikube ou Azure CLI
- Helm 3
- kubectl
- Terraform (pour Azure)
- Java 17+ (pour dÃ©veloppement local)
- Node.js 20+ (pour dÃ©veloppement local)

## ğŸ—ï¸ Architecture

- **Backend**: Spring Boot (Java 17) avec API REST, RabbitMQ, Elasticsearch et PostgreSQL
- **Frontend**: Angular 17 avec sections RabbitMQ, Elasticsearch et PostgreSQL
- **PostgreSQL**: Base de donnÃ©es avec gestion des utilisateurs
- **RabbitMQ**: Message broker avec interface admin
- **Elasticsearch**: Moteur de recherche et stockage de logs
- **Logstash**: Pipeline d'ingestion de logs
- **Kibana**: Interface de visualisation Elasticsearch

## ğŸ“š Documentation

- [âš¡ QUICKREF.md](QUICKREF.md) - **RÃ©fÃ©rence rapide des commandes**
- [ğŸš€ AZURE.md](AZURE.md) - Guide complet Azure AKS avec Terraform
- [ğŸ˜ POSTGRESQL.md](POSTGRESQL.md) - Documentation PostgreSQL et API users
- [ğŸ› ï¸ TROUBLESHOOTING.md](TROUBLESHOOTING.md) - RÃ©solution des problÃ¨mes courants
- [ğŸ›¡ï¸ PREVENTION.md](PREVENTION.md) - **Comment Ã©viter les problÃ¨mes de cache**

## ğŸ’° CoÃ»ts Azure (Configuration Optimale)

**Configuration RecommandÃ©e: ~22-25â‚¬/mois**
- **VM**: Standard_B2s (2 vCPU, 4 GB RAM) - ~22â‚¬/mois
- **AKS**: Free tier - 0â‚¬
- **Services**: Tous en ClusterIP (pas de LoadBalancer) - 0â‚¬
- **Stockage + Bande passante**: ~3-5â‚¬/mois
- **AccÃ¨s**: Via tunnel SSH/kubectl port-forward

**Alternatives:**
- Standard_B1s (1 vCPU, 1 GB): ~10â‚¬/mois - Trop juste pour ELK
- Standard_B2s_v2 (2 vCPU, 8 GB): ~30â‚¬/mois - Marge confortable
- Standard_D2s_v3 (2 vCPU, 8 GB): ~35â‚¬/mois - Meilleure performance

## ğŸš€ DÃ©ploiement Azure AKS

### 1. Configuration Terraform

Ã‰ditez `terraform/terraform.tfvars` avec vos informations:
```bash
ghcr_username = "votre-username-github"
ghcr_token    = "ghp_votre_token_github"
```

### 2. CrÃ©er l'infrastructure

```bash
cd terraform
terraform init
terraform plan
terraform apply
```

### 3. Construire et publier les images

```bash
./build-images.sh
```

### 4. DÃ©ployer l'application

```bash
./azure-deploy.sh
```

### 5. AccÃ©der aux services via tunnel

```bash
./tunnel.sh
```

Cette commande crÃ©e des tunnels vers:
- **Frontend**: http://localhost:8080
- **Backend API**: http://localhost:8081
- **RabbitMQ Admin**: http://localhost:15672 (guest/guest)
- **Kibana**: http://localhost:5601

Appuyez sur `Ctrl+C` pour arrÃªter les tunnels.

## ğŸš‡ Utilisation du tunnel

Le script `tunnel.sh` remplace les LoadBalancers coÃ»teux (~36â‚¬/mois) par des tunnels SSH gratuits:

```bash
# DÃ©marrer tous les tunnels
./tunnel.sh

# Dans un autre terminal, vous pouvez aussi crÃ©er des tunnels individuels
kubectl port-forward service/hello-world-frontend-service 8080:80
kubectl port-forward service/rabbitmq-service 15672:15672
kubectl port-forward service/kibana-service 5601:5601
```

## ğŸš€ DÃ©ploiement Minikube (Local)

### 1. DÃ©marrer Minikube

```bash
minikube start
```

### 2. Construire et charger les images Docker

```bash
chmod +x build-images.sh
./build-images.sh
```

Cette commande:
- Construit l'image Docker du backend
- Construit l'image Docker du frontend
- Charge les images dans Minikube

### 3. DÃ©ployer avec Helm

```bash
chmod +x deploy.sh
./deploy.sh
```

Cette commande:
- Installe ou met Ã  jour le chart Helm
- Attend que les pods soient prÃªts
- Affiche l'Ã©tat du dÃ©ploiement

### 4. AccÃ©der Ã  l'application

Option 1 - Via Minikube service:
```bash
minikube service hello-world-frontend-service
```

Option 2 - Via port-forward:
```bash
kubectl port-forward service/hello-world-frontend-service 8081:80
```
Puis ouvrez http://localhost:8081 dans votre navigateur.

## ğŸ“Š Commandes utiles

### VÃ©rifier le statut
```bash
chmod +x status.sh
./status.sh
```

### Voir les logs en temps rÃ©el
```bash
# Backend
kubectl logs -f -l app=hello-world-backend

# Frontend
kubectl logs -f -l app=hello-world-frontend
```

### RedÃ©marrer les pods
```bash
kubectl rollout restart deployment/hello-world-backend
kubectl rollout restart deployment/hello-world-frontend
```

### Nettoyer le dÃ©ploiement
```bash
chmod +x cleanup.sh
./cleanup.sh
```

## ğŸ”§ DÃ©veloppement local

### Backend

```bash
cd backend
./mvnw spring-boot:run
```

L'API sera disponible sur http://localhost:8080/api/hello

### Frontend

```bash
cd frontend
npm install
npm start
```

L'application sera disponible sur http://localhost:4200

## ğŸ¨ Structure du projet

```
hello-world/
â”œâ”€â”€ backend/                    # Application Spring Boot
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/                   # Application Angular
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ helm/                       # Chart Helm
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.yaml
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ backend-deployment.yaml
â”‚       â”œâ”€â”€ backend-service.yaml
â”‚       â”œâ”€â”€ frontend-deployment.yaml
â”‚       â””â”€â”€ frontend-service.yaml
â”œâ”€â”€ build-images.sh            # Script de build
â”œâ”€â”€ deploy.sh                  # Script de dÃ©ploiement
â”œâ”€â”€ cleanup.sh                 # Script de nettoyage
â”œâ”€â”€ status.sh                  # Script de statut
â””â”€â”€ README.md
```

## ğŸ” Configuration Helm

Le chart Helm peut Ãªtre personnalisÃ© via `helm/values.yaml`:

```yaml
backend:
  replicaCount: 1              # Nombre de rÃ©plicas backend
  image:
    repository: hello-backend
    tag: latest

frontend:
  replicaCount: 1              # Nombre de rÃ©plicas frontend
  service:
    nodePort: 30080           # Port NodePort
```

### DÃ©ployer avec des valeurs personnalisÃ©es

```bash
helm upgrade --install hello-world ./helm \
  --set backend.replicaCount=2 \
  --set frontend.replicaCount=2
```

## ğŸ› DÃ©pannage

### Les pods ne dÃ©marrent pas

```bash
kubectl describe pod <pod-name>
kubectl logs <pod-name>
```

### Les images ne sont pas trouvÃ©es

VÃ©rifiez que les images sont bien dans Minikube:
```bash
minikube image ls | grep hello
```

Si besoin, rechargez-les:
```bash
./build-images.sh
```

### Le frontend ne peut pas contacter le backend

VÃ©rifiez que le service backend est accessible:
```bash
kubectl get svc hello-backend-service
kubectl exec -it <frontend-pod> -- curl http://hello-backend-service:8080/api/hello
```

## ğŸ“¦ Reconstruire et redÃ©ployer

### Minikube (local)
```bash
./build-images.sh
./deploy.sh
```

### Azure AKS
```bash
# Rebuilder et pousser les images (--no-cache automatique)
./build-and-push-azure.sh

# Recharger les images sur le cluster
./azure-reload-images.sh

# VÃ©rifier le statut
./azure-status.sh
```

## âš ï¸ ProblÃ¨mes frÃ©quents et solutions

### Cache Docker qui empÃªche les changements

**SymptÃ´me**: Modifications de code non visibles aprÃ¨s rebuild

**Solutions**:
- **Minikube**: Utilisez `./build-images.sh` (--no-cache automatique)
- **Azure**: Utilisez `./build-and-push-azure.sh` (--no-cache automatique)
- Consultez [TROUBLESHOOTING.md](TROUBLESHOOTING.md) pour plus de dÃ©tails

### Rollout restart Ã©choue sur Azure (Insufficient CPU)

**SymptÃ´me**: `kubectl rollout restart` timeout avec erreur CPU

**Solution**: Utilisez `./azure-reload-images.sh` qui supprime/recrÃ©e les pods un par un

### Image non mise Ã  jour sur Azure

**Cause**: Cache buildx multi-platform

**Solution**: Le flag `--no-cache` est maintenant automatique dans `build-and-push-azure.sh`

ğŸ“– **Guide complet**: Consultez [TROUBLESHOOTING.md](TROUBLESHOOTING.md)

## ğŸ›‘ ArrÃªter l'application

```bash
./cleanup.sh
minikube stop
```

## ğŸ“ Notes

- Le backend expose une API REST sur `/api/hello`
- Le frontend appelle automatiquement le backend au dÃ©marrage
- Les images Docker utilisent le multi-stage build pour optimiser la taille
- Les health checks sont configurÃ©s pour Kubernetes (liveness et readiness probes)

## ğŸ¯ Endpoints

- Frontend: http://<minikube-ip>:30080
- Backend API: http://hello-backend-service:8080/api/hello (interne au cluster)
